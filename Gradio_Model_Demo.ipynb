{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWCZqlpI37o93mMOhB2UQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishalDhungana/Reptiles-Amphibians-Classifier/blob/main/Gradio_Model_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fYsM_cV8JSsA"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import keras.layers as layers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Model\n",
        "from numpy import argmax\n",
        "from keras.optimizers import SGD, Adam\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import gradio as gr\n",
        "import random\n",
        "import shutil\n",
        "from keras.applications import EfficientNetB0,MobileNetV2,VGG19\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip folder\n",
        "file_name = \"new-reptiles-and-amphibians-image-dataset.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done unzipping')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1k_t2E8J06v",
        "outputId": "d5cc8392-d0f9-4405-c363-733877eacddf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done unzipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE GRADIO INTERFACE FOR IMAGE PREDICTION DEMO\n",
        "\n",
        "IMAGE_SIZE=(224,224) # images will be resized to this\n",
        "def build_and_compile_cnn():\n",
        "    \"\"\"\n",
        "    Build and compile a Convolutional Neural Network model.\n",
        "\n",
        "    The CNN consists of:\n",
        "    - Three convolutional layers with ReLU activation and max pooling layers.\n",
        "    - A flattening layer to convert 2D matrices into a 1D vector.\n",
        "    - A dense (fully connected) layer with ReLU activation.\n",
        "    - A dense output layer with sigmoid activation for binary classification.\n",
        "\n",
        "    The model is compiled using the RMSProp optimizer and binary cross-entropy loss.\n",
        "\n",
        "    Returns:\n",
        "        model (Sequential): The compiled CNN model.\n",
        "    \"\"\"\n",
        "    # Initialize a sequential model\n",
        "    model = Sequential([\n",
        "        # 1: convolutional layer with 32 filters of size 3x3, ReLU activation, and a max pooling layer with window size 2x2\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        # 2: convolutional layer with 64 filters of size 3x3, ReLU activation, and a max pooling layer with window size 2x2\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        # 3: convolutional layer with 64 filters of size 3x3, ReLU activation, and a max pooling layer with window size 2x2\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        # 4: flatten pixels of resulting image into a 1D vector\n",
        "        layers.Flatten(),\n",
        "        # 5: feed result into fully connected dense layer with 64 units and ReLU activation\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        # 6. output layer with a single unit and sigmoid activation for binary classification\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # optimizerAdam = Adam(learning_rate=ALPHA) # RMSProp gives much better results\n",
        "    # Use binary cross-entropy loss function and RMSProp optimizer\n",
        "    model.compile(loss='binary_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create a new instance of the raw model by loading saved weights\n",
        "raw_model = build_and_compile_cnn()\n",
        "raw_model.load_weights('16_epochs.weights.h5')\n",
        "\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES=2\n",
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # prepare EfficientNetB0\n",
        "    efficientnet = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "    efficientnet.trainable = False\n",
        "    x1 = efficientnet.output\n",
        "    x1 = layers.GlobalAveragePooling2D()(x1)\n",
        "    x1 = layers.BatchNormalization()(x1)\n",
        "    x1 = layers.Dropout(0.2)(x1)\n",
        "\n",
        "    # prepare MobileNetV2\n",
        "    mobilenet = MobileNetV2(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "    mobilenet.trainable = False\n",
        "    x2 = mobilenet.output\n",
        "    x2 = layers.GlobalAveragePooling2D()(x2)\n",
        "    x2 = layers.BatchNormalization()(x2)\n",
        "    x2 = layers.Dropout(0.2)(x2)\n",
        "\n",
        "    # prepare VGG19\n",
        "    VGG = VGG19(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "    VGG.trainable = False\n",
        "    x3 = mobilenet.output\n",
        "    x3 = layers.GlobalAveragePooling2D()(x3)\n",
        "    x3 = layers.BatchNormalization()(x3)\n",
        "    x3 = layers.Dropout(0.2)(x3)\n",
        "\n",
        "    # Concatenate the 3 layers\n",
        "    concatenated = layers.concatenate([x1, x2,x3])\n",
        "\n",
        "\n",
        "    #classification layer\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(concatenated)\n",
        "\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Create a new instance of the ensemble model by loading saved weights\n",
        "ensemble_model = build_model(NUM_CLASSES)\n",
        "ensemble_model.load_weights('16_epochs_Ensemble.weights.h5')\n",
        "\n",
        "# Create df with image paths and labels\n",
        "image_dir = Path(\"new-reptiles-and-amphibians-image-dataset\")\n",
        "filepaths = list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "image_df = pd.concat([filepaths, labels], axis=1) # 5994 rows, 2 cols\n",
        "# Select 10 Reptile and Amphibian images to show in the Gradio interface\n",
        "def get_sample_image_paths():\n",
        "    \"\"\"\n",
        "    Get sample reptile/amphibian images\n",
        "    \"\"\"\n",
        "    # Select 20 random images (10 Reptile, 10 Amphibian) to show in Gradio\n",
        "    reptiles = image_df[image_df['Label'] == 'Reptile'].sample(10, random_state=13)\n",
        "    amphibians = image_df[image_df['Label'] == 'Amphibian'].sample(10, random_state=13)\n",
        "    sample_df = pd.concat([reptiles, amphibians])\n",
        "    sample_df = sample_df.sample(frac=1).reset_index(drop=True) # shuffle df\n",
        "    return sample_df.values.tolist()\n",
        "\n",
        "# Create function for Gradio interface\n",
        "def classify_image_raw_model(inp):\n",
        "    \"\"\"\n",
        "    Classify an input image as either 'Amphibian' or 'Reptile' using the raw CNN Model\n",
        "    \"\"\"\n",
        "    # Resize and normalize input image\n",
        "    inp = inp.resize(IMAGE_SIZE)\n",
        "    inp = np.array(inp) / 255.0\n",
        "    inp = inp.reshape((-1, 224, 224, 3))\n",
        "\n",
        "    # Predict class\n",
        "    prediction = raw_model.predict(inp)\n",
        "    predicted_class = int(np.round(prediction[0][0]))\n",
        "\n",
        "    # Map class index to class name\n",
        "    class_names = {0: 'Amphibian', 1: 'Reptile'}\n",
        "    return class_names[predicted_class]\n",
        "\n",
        "def classify_image_ensemble_model(inp):\n",
        "    \"\"\"\n",
        "    Classify an input image as either 'Amphibian' or 'Reptile' using the ensembled pre-trained CNN model\n",
        "    \"\"\"\n",
        "    # Resize and normalize input image\n",
        "    inp = inp.resize(IMAGE_SIZE)\n",
        "    inp = np.array(inp) / 255.0\n",
        "    inp = inp.reshape((-1, 224, 224, 3)) # For this function, replace any of these to fit the pre-trained model params\n",
        "\n",
        "    # Predict class\n",
        "    prediction = ensemble_model.predict(inp) # For this function, replace this with the ensemble model\n",
        "    predicted_class = int(np.argmax(prediction[0]))\n",
        "\n",
        "    # Map class index to class name\n",
        "    class_names = {0: 'Amphibian', 1: 'Reptile'}\n",
        "    return class_names[predicted_class]\n",
        "\n",
        "def classify_images(inp, true_label):\n",
        "    \"\"\"\n",
        "    Classify an input image using both models and return both results\n",
        "    \"\"\"\n",
        "    raw_model_pred = classify_image_raw_model(inp)\n",
        "    ensemble_model_pred = classify_image_ensemble_model(inp)\n",
        "    return raw_model_pred, ensemble_model_pred, true_label\n",
        "\n",
        "# Create a Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_images,\n",
        "    inputs=[gr.Image(type=\"pil\"), gr.Textbox(label=\"True Label\")],\n",
        "    outputs=[\n",
        "        gr.Label(num_top_classes=1, label=\"Raw Classifier\"),\n",
        "        gr.Label(num_top_classes=1, label=\"Ensembled Pre-trained Classifier\")\n",
        "    ],\n",
        "    examples=get_sample_image_paths(), # this gives us images and their true labels\n",
        "    title=\"Amphibian vs Reptile Classifier\",\n",
        "    description=\"Upload an image and specify its true label, or click one of the examples below. Then, click 'Submit' to see whether the models classify it as an Amphibian or a Reptile.\"\n",
        ")\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "NaJ-7vHQsQTW",
        "outputId": "3f3a47b4-421c-48e7-e86f-980c7eedd3ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "<ipython-input-37-b1861eec70aa>:62: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet = MobileNetV2(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ffa3cd3b41372256fa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ffa3cd3b41372256fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}